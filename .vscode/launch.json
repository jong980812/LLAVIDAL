{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Train LLAVIDAL with torchrun",
      "type": "debugpy",
      "request": "launch",
      "program": "/data/jongseo/anaconda3/envs/llava-video/bin/deepspeed",
      "justMyCode": false,
      "args": [
        "--num_nodes", "1",
      "--num_gpus", "1",
        "--master_port", "44444",
        "llavidal/train/train_mem.py",
        "--deepspeed", "scripts/zero2.json",
        "--version", "v1",
        "--tune_mm_mlp_adapter", "True",
        "--mm_use_vid_start_end",
        "--bf16", "True",
        "--num_train_epochs", "3",
        "--per_device_train_batch_size", "4",
        "--per_device_eval_batch_size", "4",
        "--gradient_accumulation_steps", "1",
        "--evaluation_strategy", "no",
        "--save_strategy", "steps",
        "--save_steps", "5",
        // "--report_to","wandb",
        "--save_total_limit", "3",
        "--learning_rate", "2e-5",
        "--weight_decay", "0.",
        "--warmup_ratio", "0.03",
        "--lr_scheduler_type", "cosine",
        "--logging_steps", "100",
        "--tf32", "True",
        "--save_only_model", "True",
        "--model_max_length", "2048",
        "--gradient_checkpointing", "True",
        "--lazy_preprocess", "True",
        "--output_dir", "/data/jongseo/project/vlm/LLAVIDAL/work_dirs/deubg",
        "--model_name_or_path", "mmaaz60/LLaVA-7B-Lightening-v1-1",
        "--data_path", "/data/dataset/ADL-X/instruction_data/NTU_QA-for-training.json",
        "--video_folder", "/local_datasets/ADL-X/data/vidlab_datasets/NTU_combination_video_features",
        "--object_folder", "/local_datasets/ADL-X/data/users/rchakra6/concatenated_objects",
        "--pose_folder", "/local_datasets/ADL-X/data/users/rchakra6/concatenated_poses",
        "--sample_ratio","0.01"
      ],
      "env": {
        "HF_HOME": "/data/dataset/LLaVA-Video-100K-Subset/",
        "PYTHONPATH":"./:$PYTHONPATH",
        "WANDB_PROJECT": "LLAVIDAL",
        "WANDB_NAME": "LLAVIDAL_videe-text_3epochs",
      },
      "console": "integratedTerminal"
    }
    ,
      {
      "name": "Evaluatioj LLAVIDAL ADL-MC",
      "type": "debugpy",
      "request": "launch",
      "program": "/data/jongseo/anaconda3/envs/llava-video/bin/torchrun",
      "justMyCode": false,
      "args": [
        "--nproc_per_node","1",
        "llavidal/eval/run_inference_action_recognition_charades.py",
        "--video_dir", "/data2/local_datasets/Charades_v1_480",
        "--qa_file", "/data/jongseo/project/vlm/LLAVIDAL/evaluation_ressults/anno_evaluation/ADLMCQ-AR-Charades.json",
        "--output_dir", "/data/jongseo/project/vlm/LLAVIDAL/evaluation_ressults/results",
        "--output_name", "charades_results",
        "--model-name", "mmaaz60/LLaVA-7B-Lightening-v1-1",
        "--conv-mode", "llavidal_v1",
        "--projection_path","/data/dataset/ADL-X/model_weights/llavidal_weights.bin"
        // "--projection_path", "/data/jongseo/project/vlm/LLAVIDAL/work_dirs/LLAVIDAL_videe-text_3epochs_multinode16gpus_zero2/mm_projector.bin"
      ],
      "env": {
        "HF_HOME": "/data/dataset/LLaVA-Video-100K-Subset/",
        "PYTHONPATH":"./:$PYTHONPATH",
      },
      "console": "integratedTerminal"
    }
  ]
}